{
  "name": "0Dinit_10V0Dupdate_DTE_batch24_LRGD1e-5D1e-4on10K_Range5e3_DecompClippedNoisedPatchD_debug" //  please remove "debug_" during training
  , "use_tb_logger": false
  , "model":"srragan"
  , "scale": 4
  , "gpu_ids": [0]
  , "range": [0,1]

  , "datasets": {
    "train": {
      "name": "DIV2K"
      , "mode": "LRHR"
      , "dataroot_HR": "DIV2K_train/DIV2K_train_sub_HRx4.lmdb"
      , "dataroot_LR": "DIV2K_train/DIV2K_train_sub_bicLRx4.lmdb"
      , "subset_file": null
      , "use_shuffle": true
      , "n_workers": 8
      , "batch_size": 6
      , "batch_size_4_grads_G": 24
      , "batch_size_4_grads_D": 24
      , "HR_size": 256
      , "use_flip": true
      , "use_rot": true
    }
    , "val": {
      "name": "val_set14_part"
      , "mode": "LRHR"
      , "dataroot_HR": "Set14/Set14_HRx4" //"DIV2K_valid/DIV2K_valid_HRx4"//"Set14"
      , "dataroot_LR": "Set14/Set14_bicLRx4" //,"DIV2K_valid/DIV2K_valid_bicLRx4"//,"Set14_bicLRx4"
    }
  }

  , "path": {
    "root": "/home/ybahat/PycharmProjects/SRGAN"
    , "pretrain_model_G_disabled": "../experiments/debug_002_RRDB_ESRGAN_x4_DIV2K_archived_181211-150404/models/2008_G.pth"
    //, "pretrain_model_G": "../pretrained_models/SRResNet_bicx4_in3nf64nb16.pth" //,"pretrained_models/RRDB_PSNR_x4.pth"
    , "pretrain_model_G": "../pretrained_models/RRDB_PSNR_x4.pth" //,"pretrained_models/RRDB_PSNR_x4.pth"
}

  , "network_G": {
    "which_model_G": "RRDB_net" // RRDB_net | sr_resnet
    , "DTE_arch": 1
    , "sigmoid_range_limit": 0
    , "noise_input": "None" // "all_layers" //"all_layers","first_layer","None"
    , "norm_type": null
    , "mode": "CNA"
    , "nf": 64
    , "nb": 23
    , "in_nc": 3
    , "out_nc": 3
    , "gc": 32
    , "group": 1
  }
  , "network_D": {
    "which_model_D": "PatchGAN" //,"discriminator_vgg_128"
    , "relativistic": 1
    , "decomposed_input": 1
    , "pre_clipping": 1
    , "norm_type": "batch"
    , "act_type": "leakyrelu"
    , "mode": "CNA"
    , "n_layers": 3
    , "nf": 64
    , "in_nc": 3
  }

  , "train": {
    "resume": 1
    ,"lr_G": 1e-5 //1e-4
    , "weight_decay_G": 0
    , "beta1_G": 0.9
    , "lr_D": 1e-4 //1e-4
    , "weight_decay_D": 0
    , "beta1_D": 0.9
    , "lr_scheme": "MultiStepLR"
    , "lr_steps": [50000, 100000, 200000, 300000] //[5000, 10000, 20000, 30000] //
    , "lr_gamma": 0.5
    , "pixel_domain": "HR"
    , "pixel_criterion": "l1"
    , "pixel_weight": 0 //1e-2
    , "feature_domain": "HR"
    , "feature_criterion": "l1"
    , "feature_weight": 0 //1
    , "gan_type": "lsgan" //,"vanilla"
    , "gan_weight": 1 //5e-3
    , "min_D_prob_ratio_4_G": 1.05
    , "range_weight": 5000
    , "D_update_ratio": 0
    , "D_valid_Steps_4_G_update": 10 //Perform G steps using l_gan only when D was successfull in previous D_update_ratio steps
    , "DTE_exp": 1 //Means I use loss mask and padding while training and testing, no matter if DTE_arch or not.

    //for wgan-gp
    // , "D_update_ratio": 1
     , "D_init_iters": 0
    // , "gp_weigth": 10

    , "manual_seed": 0
    , "niter": 500100 //5e5
    , "val_freq": 50 //5e3
  }

  , "logger": {
    "print_freq": 10 //200
    , "save_checkpoint_freq": 1e3 //5e3
  }
}
